---
title: "Why Explainable AI Matters for Financial Regulation"
description: "The case for transparency in AI-driven credit and risk decisions affecting millions of Americans. Not just for compliance — for doing this right."
pubDatetime: 2025-11-15
tags: ["AI Governance", "Financial Risk", "Explainability"]
featured: true
draft: false
---

The financial services industry is going through a transformation that most people outside it do not fully appreciate. Machine learning models now make or heavily influence decisions about who gets a mortgage, who pays what interest rate, who gets approved for a car loan, and who gets flagged as a fraud risk. These are not edge cases or pilot programs. They are the operating reality of how American financial institutions make decisions about hundreds of millions of people today.

And most of these models are black boxes.

Not in the sense that they are intentionally opaque. In the sense that even the teams that built them often cannot fully explain why a specific individual was denied or approved. The model learned patterns from historical data that map inputs to outputs through mathematical relationships too complex for human intuition to follow. It works in aggregate. It falls apart as explanation for any individual case.

That is a problem. It is a growing legal problem, a regulatory problem, and most importantly, a fairness problem.

## The adverse action notice gap

When a bank denies your mortgage application, you have a legal right to know why. The Equal Credit Opportunity Act and the Fair Credit Reporting Act both require lenders to provide adverse action notices: specific, substantive reasons for the denial. Not "your application did not meet our criteria." Specific reasons.

This requirement was written for a world of human underwriters and rule-based scorecards. It is being applied to a world of gradient-boosted ensembles with 500 features, deep learning models trained on alternative data, and stacked model architectures where the output of one model becomes the input to another.

<figure style="margin: 2rem 0;">
  <figcaption style="font-size: 0.78rem; opacity: 0.65; margin-bottom: 0.75rem; font-style: italic;">Share of decisioning using complex ML models by financial product category — 2024</figcaption>
  <div style="display: flex; flex-direction: column; gap: 0.7rem;">
    <div>
      <div style="display: flex; justify-content: space-between; font-size: 0.8rem; margin-bottom: 0.25rem;"><span>Fraud detection</span><span style="opacity:0.65;">84%</span></div>
      <div style="background: var(--muted); border-radius: 3px; height: 11px;"><div style="width: 84%; height: 100%; background: var(--accent); border-radius: 3px;"></div></div>
    </div>
    <div>
      <div style="display: flex; justify-content: space-between; font-size: 0.8rem; margin-bottom: 0.25rem;"><span>AML / sanctions screening</span><span style="opacity:0.65;">71%</span></div>
      <div style="background: var(--muted); border-radius: 3px; height: 11px;"><div style="width: 71%; height: 100%; background: var(--accent); border-radius: 3px;"></div></div>
    </div>
    <div>
      <div style="display: flex; justify-content: space-between; font-size: 0.8rem; margin-bottom: 0.25rem;"><span>Credit card approval</span><span style="opacity:0.65;">63%</span></div>
      <div style="background: var(--muted); border-radius: 3px; height: 11px;"><div style="width: 63%; height: 100%; background: var(--accent); border-radius: 3px;"></div></div>
    </div>
    <div>
      <div style="display: flex; justify-content: space-between; font-size: 0.8rem; margin-bottom: 0.25rem;"><span>Auto lending</span><span style="opacity:0.65;">51%</span></div>
      <div style="background: var(--muted); border-radius: 3px; height: 11px;"><div style="width: 51%; height: 100%; background: var(--accent); border-radius: 3px;"></div></div>
    </div>
    <div>
      <div style="display: flex; justify-content: space-between; font-size: 0.8rem; margin-bottom: 0.25rem;"><span>Mortgage underwriting</span><span style="opacity:0.65;">34%</span></div>
      <div style="background: var(--muted); border-radius: 3px; height: 11px;"><div style="width: 34%; height: 100%; background: var(--accent); border-radius: 3px;"></div></div>
    </div>
  </div>
  <p style="font-size: 0.72rem; opacity: 0.6; margin-top: 0.6rem; font-style: italic;">Sources: McKinsey Global Banking Annual Review; OCC Model Risk Survey 2024.</p>
</figure>

The gap between what the law requires and what current model architectures can easily provide is not a small one. Regulators at the CFPB, OCC, and Federal Reserve are actively grappling with it. Banks are navigating it with varying degrees of rigor. And the individuals being denied credit are left with adverse action notices that often technically comply with the letter of the law while failing entirely to provide the substantive explanation they are entitled to.

## What explainability actually means in practice

Explainability in AI is not one thing. It is a spectrum of techniques with different uses, different costs, and different audiences. Understanding this matters because "we need explainable AI" is often invoked without clarity on what level of explainability is actually needed for what purpose.

<figure style="margin: 2rem 0;">
  <figcaption style="font-size: 0.78rem; opacity: 0.65; margin-bottom: 0.75rem; font-style: italic;">Three levels of AI explainability and their regulatory relevance</figcaption>
  <div style="border: 1px solid var(--border); border-radius: 8px; overflow: hidden;">
    <div style="padding: 1rem; border-bottom: 1px solid var(--border);">
      <div style="display: flex; align-items: flex-start; gap: 0.75rem; margin-bottom: 0.5rem;">
        <div style="font-size: 0.7rem; font-weight: 700; padding: 0.2rem 0.55rem; border-radius: 4px; background: #60a5fa20; color: #60a5fa; white-space: nowrap;">Regulatory: Medium</div>
        <div style="font-size: 0.85rem; font-weight: 600;">Global interpretability</div>
      </div>
      <p style="font-size: 0.8rem; opacity: 0.85; margin: 0 0 0.3rem;">Overall model logic — which features matter most, what patterns were learned across the full dataset.</p>
      <p style="font-size: 0.75rem; opacity: 0.6; margin: 0;"><strong>Use:</strong> Internal model validation, model documentation for regulators, model risk management frameworks.</p>
    </div>
    <div style="padding: 1rem; border-bottom: 1px solid var(--border); background: color-mix(in srgb, var(--accent) 3%, transparent);">
      <div style="display: flex; align-items: flex-start; gap: 0.75rem; margin-bottom: 0.5rem;">
        <div style="font-size: 0.7rem; font-weight: 700; padding: 0.2rem 0.55rem; border-radius: 4px; background: color-mix(in srgb, var(--accent) 20%, transparent); color: var(--accent); white-space: nowrap;">Regulatory: High</div>
        <div style="font-size: 0.85rem; font-weight: 600;">Local interpretability</div>
      </div>
      <p style="font-size: 0.8rem; opacity: 0.85; margin: 0 0 0.3rem;">Individual decision explanation — why was this specific applicant denied? Which features drove the outcome for this person?</p>
      <p style="font-size: 0.75rem; opacity: 0.6; margin: 0;"><strong>Use:</strong> Adverse action notices, customer disputes, fair lending examinations, consumer remediation.</p>
    </div>
    <div style="padding: 1rem;">
      <div style="display: flex; align-items: flex-start; gap: 0.75rem; margin-bottom: 0.5rem;">
        <div style="font-size: 0.7rem; font-weight: 700; padding: 0.2rem 0.55rem; border-radius: 4px; background: #22c55e20; color: #22c55e; white-space: nowrap;">Regulatory: Very High</div>
        <div style="font-size: 0.85rem; font-weight: 600;">Counterfactual explanations</div>
      </div>
      <p style="font-size: 0.8rem; opacity: 0.85; margin: 0 0 0.3rem;">What would need to change to get a different outcome? "Your debt-to-income ratio of 46% exceeded our threshold. Reducing it below 43% would likely change the outcome."</p>
      <p style="font-size: 0.75rem; opacity: 0.6; margin: 0;"><strong>Use:</strong> Actionable consumer guidance, regulatory examinations, fair lending corrective action plans.</p>
    </div>
  </div>
</figure>

For regulatory compliance, local interpretability and counterfactual explanations are what matter most. Consumers and examiners do not need to understand the full model architecture. They need to understand specific decisions and what, concretely, could change them.

This is where SHAP values become practically important. SHAP allows you to decompose a model's prediction into the contribution of each input feature for a specific observation. You can tell a specific applicant that their application was denied primarily because their debt-to-income ratio exceeded a threshold, that a shorter employment tenure also contributed, and that improving those two factors would likely change the outcome. That is what a substantive adverse action notice looks like.

## The accuracy-explainability tradeoff is mostly a myth

One of the most persistent misunderstandings in this space is the idea that you have to choose between accurate models and explainable models. That if you want a model regulators can understand, you have to sacrifice performance. This was somewhat true ten years ago. It is largely not true now.

<figure style="margin: 2rem 0;">
  <figcaption style="font-size: 0.78rem; opacity: 0.65; margin-bottom: 0.75rem; font-style: italic;">AUC performance vs. explainability — common credit model architectures</figcaption>
  <div style="border: 1px solid var(--border); border-radius: 8px; overflow: hidden;">
    <div style="display: grid; grid-template-columns: 1fr auto auto; border-bottom: 1px solid var(--border);">
      <div style="padding: 0.5rem 0.75rem; font-size: 0.72rem; font-weight: 700; opacity: 0.6; border-right: 1px solid var(--border);">MODEL ARCHITECTURE</div>
      <div style="padding: 0.5rem 0.75rem; font-size: 0.72rem; font-weight: 700; opacity: 0.6; text-align: center; border-right: 1px solid var(--border); min-width: 80px;">AUC</div>
      <div style="padding: 0.5rem 0.75rem; font-size: 0.72rem; font-weight: 700; opacity: 0.6; text-align: center; min-width: 110px;">EXPLAINABILITY</div>
    </div>
    <div style="display: grid; grid-template-columns: 1fr auto auto; border-bottom: 1px solid var(--border);">
      <div style="padding: 0.6rem 0.75rem; font-size: 0.8rem; border-right: 1px solid var(--border);">Scorecard / logistic regression</div>
      <div style="padding: 0.6rem 0.75rem; font-size: 0.8rem; text-align: center; border-right: 1px solid var(--border);">0.74</div>
      <div style="padding: 0.6rem 0.75rem; text-align: center;"><span style="font-size: 0.72rem; font-weight: 600; color: #22c55e;">Very High</span></div>
    </div>
    <div style="display: grid; grid-template-columns: 1fr auto auto; border-bottom: 1px solid var(--border);">
      <div style="padding: 0.6rem 0.75rem; font-size: 0.8rem; border-right: 1px solid var(--border);">Decision tree (shallow)</div>
      <div style="padding: 0.6rem 0.75rem; font-size: 0.8rem; text-align: center; border-right: 1px solid var(--border);">0.76</div>
      <div style="padding: 0.6rem 0.75rem; text-align: center;"><span style="font-size: 0.72rem; font-weight: 600; color: #22c55e;">High</span></div>
    </div>
    <div style="display: grid; grid-template-columns: 1fr auto auto; border-bottom: 1px solid var(--border); background: color-mix(in srgb, var(--accent) 4%, transparent);">
      <div style="padding: 0.6rem 0.75rem; font-size: 0.8rem; border-right: 1px solid var(--border);">XGBoost + SHAP explanations</div>
      <div style="padding: 0.6rem 0.75rem; font-size: 0.8rem; text-align: center; border-right: 1px solid var(--border); font-weight: 700; color: var(--accent);">0.81</div>
      <div style="padding: 0.6rem 0.75rem; text-align: center;"><span style="font-size: 0.72rem; font-weight: 600; color: #22c55e;">High</span></div>
    </div>
    <div style="display: grid; grid-template-columns: 1fr auto auto; border-bottom: 1px solid var(--border);">
      <div style="padding: 0.6rem 0.75rem; font-size: 0.8rem; border-right: 1px solid var(--border);">Random forest (unmodified)</div>
      <div style="padding: 0.6rem 0.75rem; font-size: 0.8rem; text-align: center; border-right: 1px solid var(--border);">0.80</div>
      <div style="padding: 0.6rem 0.75rem; text-align: center;"><span style="font-size: 0.72rem; font-weight: 600; color: #f59e0b;">Medium</span></div>
    </div>
    <div style="display: grid; grid-template-columns: 1fr auto auto;">
      <div style="padding: 0.6rem 0.75rem; font-size: 0.8rem; border-right: 1px solid var(--border);">Deep learning (black box)</div>
      <div style="padding: 0.6rem 0.75rem; font-size: 0.8rem; text-align: center; border-right: 1px solid var(--border);">0.83</div>
      <div style="padding: 0.6rem 0.75rem; text-align: center;"><span style="font-size: 0.72rem; font-weight: 600; color: #ef4444;">Very Low</span></div>
    </div>
  </div>
  <p style="font-size: 0.72rem; opacity: 0.6; margin-top: 0.5rem; font-style: italic;">AUC differences between architectures are often smaller than the uncertainty in training data. Compliance and regulatory defense costs for black-box models are significantly larger.</p>
</figure>

A well-tuned logistic regression or scorecard model, with careful feature engineering, often comes within two or three AUC points of a complex ensemble on credit scoring tasks. Meanwhile, the cost of compliance, explainability, and regulatory defense for the complex model can be substantially higher. Sometimes the most accurate model accounting for total cost is the interpretable one.

Where complexity is genuinely needed, XGBoost with SHAP-based explainability has become a practical standard for financial applications precisely because it delivers near-state-of-the-art performance while remaining explainable at the individual decision level.

## What I am building toward

At IBM, my work embeds explainability into the model development lifecycle from the beginning. That means feature engineering with explicit regulatory justification for every input variable. It means model selection that accounts for interpretability requirements alongside accuracy benchmarks. It means automated adverse action reason generation that produces compliant, substantive explanations at scale.

<figure style="margin: 2rem 0;">
  <figcaption style="font-size: 0.78rem; opacity: 0.65; margin-bottom: 0.75rem; font-style: italic;">Explainability-first model development lifecycle</figcaption>
  <div style="display: flex; flex-direction: column; gap: 0.5rem;">
    <div style="display: flex; gap: 1rem; align-items: flex-start; padding: 0.75rem; border: 1px solid var(--border); border-radius: 6px;">
      <div style="width: 28px; height: 28px; border-radius: 50%; background: var(--accent); color: var(--background); display: flex; align-items: center; justify-content: center; font-weight: 700; font-size: 0.8rem; flex-shrink: 0;">1</div>
      <div>
        <div style="font-size: 0.82rem; font-weight: 600; margin-bottom: 0.15rem;">Define regulatory constraints upfront</div>
        <div style="font-size: 0.74rem; opacity: 0.65;">Which features are legally permissible? What explanation format does the regulation require? What are the adverse action reason code standards?</div>
      </div>
    </div>
    <div style="display: flex; gap: 1rem; align-items: flex-start; padding: 0.75rem; border: 1px solid var(--border); border-radius: 6px;">
      <div style="width: 28px; height: 28px; border-radius: 50%; background: var(--accent); color: var(--background); display: flex; align-items: center; justify-content: center; font-weight: 700; font-size: 0.8rem; flex-shrink: 0;">2</div>
      <div>
        <div style="font-size: 0.82rem; font-weight: 600; margin-bottom: 0.15rem;">Feature engineering with documented justification</div>
        <div style="font-size: 0.74rem; opacity: 0.65;">Every feature must have a clear business rationale before it enters the model. No proxy variables. No features that encode protected characteristics.</div>
      </div>
    </div>
    <div style="display: flex; gap: 1rem; align-items: flex-start; padding: 0.75rem; border: 1px solid var(--border); border-radius: 6px;">
      <div style="width: 28px; height: 28px; border-radius: 50%; background: var(--accent); color: var(--background); display: flex; align-items: center; justify-content: center; font-weight: 700; font-size: 0.8rem; flex-shrink: 0;">3</div>
      <div>
        <div style="font-size: 0.82rem; font-weight: 600; margin-bottom: 0.15rem;">Model selection with explainability as a constraint</div>
        <div style="font-size: 0.74rem; opacity: 0.65;">Complexity is a cost. Choose the architecture with that cost explicitly priced in alongside accuracy benchmarks and expected regulatory defense requirements.</div>
      </div>
    </div>
    <div style="display: flex; gap: 1rem; align-items: flex-start; padding: 0.75rem; border: 1px solid var(--border); border-radius: 6px;">
      <div style="width: 28px; height: 28px; border-radius: 50%; background: var(--accent); color: var(--background); display: flex; align-items: center; justify-content: center; font-weight: 700; font-size: 0.8rem; flex-shrink: 0;">4</div>
      <div>
        <div style="font-size: 0.82rem; font-weight: 600; margin-bottom: 0.15rem;">Validate explanations against human expert judgment</div>
        <div style="font-size: 0.74rem; opacity: 0.65;">SHAP and LIME values need to make sense to underwriters and compliance officers, not just data scientists. Validate before deployment, not after complaints.</div>
      </div>
    </div>
    <div style="display: flex; gap: 1rem; align-items: flex-start; padding: 0.75rem; border: 1px solid var(--border); border-radius: 6px;">
      <div style="width: 28px; height: 28px; border-radius: 50%; background: var(--accent); color: var(--background); display: flex; align-items: center; justify-content: center; font-weight: 700; font-size: 0.8rem; flex-shrink: 0;">5</div>
      <div>
        <div style="font-size: 0.82rem; font-weight: 600; margin-bottom: 0.15rem;">Automate adverse action reason generation at inference time</div>
        <div style="font-size: 0.74rem; opacity: 0.65;">Compliant explanations produced as part of the scoring pipeline, not assembled post-hoc. At scale, post-hoc explanation is fragile and inconsistent.</div>
      </div>
    </div>
  </div>
</figure>

The regulatory direction here is clear. The EU AI Act classifies credit scoring as high-risk AI. The CFPB has issued guidance on algorithmic adverse action notices. The OCC's model risk guidance is increasingly explicit about explainability requirements. The United States will follow Europe's lead on this. The question is timing, not direction.

Financial institutions that invest in explainable AI now are not just managing regulatory risk. They are building systems that are more trustworthy, more auditable, and fundamentally more fair. That alignment between compliance and ethics is not always available in this field. When it is, you should take it.

The argument for explainable AI in finance is not primarily a compliance argument. It is an argument about what it means to make consequential decisions about people's lives responsibly. The compliance requirement is the floor. The actual goal is higher than that.

---

*This is the first in a series on AI governance in financial services. Next: building risk intelligence systems that institutions can actually trust in production.*
